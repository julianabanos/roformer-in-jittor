{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoFormer with PyTorch Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers # Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Include Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataClassEncoder:\n",
    "    def __init__(self, \n",
    "                last_hidden_state,\n",
    "                past_key_values,\n",
    "                hidden_states,\n",
    "                attentions,\n",
    "                cross_attentions,\n",
    "                ):\n",
    "        self.last_hidden_state = last_hidden_state\n",
    "        self.past_key_values = past_key_values\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "        self.cross_attentions = cross_attentions\n",
    "\n",
    "@dataclass\n",
    "class DataClassRoformer:\n",
    "    def __init__(self,\n",
    "                last_hidden_state, # Expected: 1, 11, 768 Actual: 2, 753, 768\n",
    "                pooler_output, # Expected: 1, 768 Actual: 2, 768\n",
    "                past_key_values, # Expected: 1, 12, 11 64 Actual: 2, 12, 753, 64\n",
    "                hidden_states, # Expected: None Actual: 2, 753, 768\n",
    "                attentions, # Expected: None Actual: 2, 12, 753, 768\n",
    "                cross_attentions, # Expected: None Actual: 2, 12, 753, 64\n",
    "                ):\n",
    "        self.last_hidden_state = last_hidden_state\n",
    "        self.pooler_output = pooler_output\n",
    "        self.past_key_values = past_key_values\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "        self.cross_attentions = cross_attentions\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataClassCausalLM:\n",
    "    def __init__(self,\n",
    "                loss,\n",
    "                logits,\n",
    "                pooler_output,\n",
    "                past_key_values,\n",
    "                hidden_states,\n",
    "                attentions,\n",
    "                cross_attentions,\n",
    "                ):\n",
    "        self.loss = loss\n",
    "        self.logits = logits\n",
    "        self.pooler_output = pooler_output\n",
    "        self.past_key_values = past_key_values\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "        self.cross_attentions = cross_attentions\n",
    "\n",
    "@dataclass\n",
    "class DataClassSequenceClassifier:\n",
    "        def __init__(self,\n",
    "                        loss,\n",
    "                        logits,\n",
    "                        hidden_states,\n",
    "                        attentions,\n",
    "                        ):\n",
    "                self.loss = loss\n",
    "                self.logits = logits\n",
    "                self.hidden_states = hidden_states\n",
    "                self.attentions = attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Include the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# import jittor as jt\n",
    "# from jittor import Module\n",
    "# from jittor import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import Optional, Tuple, Callable\n",
    "import warnings\n",
    "import inspect\n",
    "from typing import List, Set, Tuple\n",
    "from dataclasses import dataclass\n",
    "from .pt_dataclasses import DataClassEncoder, DataClassRoformer, DataClassCausalLM, DataClassSequenceClassifier\n",
    "import pdb\n",
    "\n",
    "# jt.flags.use_cuda = 1\n",
    "# roformer model\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class ModelSeqClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ModelSeqClassifier, self).__init__()\n",
    "        self.roformer = Roformer(config)\n",
    "        self.classifier = ClassificationHead(config)\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                attention_mask=None, \n",
    "                token_type_ids=None, \n",
    "                head_mask=None,                \n",
    "                inputs_embeds=None, \n",
    "                labels=None, # new\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                ):\n",
    "        outputs = self.roformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # head_mask=head_mask,\n",
    "            # inputs_embeds=inputs_embeds,\n",
    "            # output_attentions=output_attentions,\n",
    "            # output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCELoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return DataClassSequenceClassifier(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ModelCausalLM, self).__init__()\n",
    "        self.roformer = Roformer(config)\n",
    "        self.cls = Cls(config)\n",
    "\n",
    "    def generate(self, input_ids, token_type_ids=None, attention_mask=None, top_p=0.95, max_length=128, do_sample=True):\n",
    "        # Assuming that the Roformer model's output can be used directly for token generation.\n",
    "        # This method generates one token at a time using top-p sampling.\n",
    "\n",
    "        # Start with the provided input_ids\n",
    "\n",
    "        generated = input_ids\n",
    "\n",
    "        # Iterate until max_length is reached\n",
    "        for _ in range(max_length):\n",
    "            # # Get the model's output\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self(input_ids=generated, token_type_ids=token_type_ids, attention_mask=attention_mask, output_attentions=False, output_hidden_states=False, return_dict=True)\n",
    "            \n",
    "            # Assume the last layer output is the logits (adjust according to your model's specifics)\n",
    "            logits = outputs.logits[:, -1, :] \n",
    "\n",
    "            # Apply top-p sampling to the logits to get the next token\n",
    "            filtered_logits = self.top_p_filtering(logits, top_p)\n",
    "            # filtered_logits = logits\n",
    "            if do_sample:\n",
    "                probabilities = nn.functional.softmax(filtered_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "            else:\n",
    "                # Use the most likely next token if do_sample is False\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1)\n",
    "\n",
    "            # Concatenate the new token to the generated sequence\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            attention_mask = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
    "                )\n",
    "            token_type_ids = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            # Stop if the sequence is getting too long\n",
    "            if generated.size(1) > max_length:\n",
    "                break\n",
    "\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        return generated\n",
    "\n",
    "    def top_p_filtering(self, logits, top_p):\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n",
    "        cumulative_probs = torch.cumsum(nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool).scatter_(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, inputs_embeds=None, encoder_hidden_states=None, \n",
    "                encoder_attention_mask=None, head_mask=None, cross_attn_head_mask=None, past_key_values=None, labels=None, \n",
    "                use_cache=None, output_attentions=True, output_hidden_states=True,): #TODO: CHECK\n",
    "        \n",
    "        # return_dict = (\n",
    "        #     return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # )\n",
    "\n",
    "        outputs = self.roformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # head_mask=head_mask,\n",
    "            # inputs_embeds=inputs_embeds,\n",
    "            # encoder_hidden_states=encoder_hidden_states,\n",
    "            # encoder_attention_mask=encoder_attention_mask,\n",
    "            # past_key_values=past_key_values,\n",
    "            # use_cache=use_cache,\n",
    "            # output_attentions=output_attentions,\n",
    "            # output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        # print \n",
    "        # pdb.set_trace()\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state #TODO:CHECK\n",
    "        # print(\"sequence_output\", sequence_output.shape)\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            lm_loss = loss_fct(\n",
    "                shifted_prediction_scores.view(-1, self.config.vocab_size),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "\n",
    "        # if not return_dict:\n",
    "        #     output = (prediction_scores,) + outputs[1:] # with pooler\n",
    "        #     return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        return DataClassCausalLM(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            pooler_output=outputs.pooler_output, # with pooler_output\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "class Roformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Roformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.config = config\n",
    "        self.dtype = torch.float16\n",
    "        self.add_pooling_layer = config.add_pooling_layer\n",
    "\n",
    "        if self.add_pooling_layer:\n",
    "            self.pooler = RoFormerPooler(config)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None,\n",
    "                use_cache=None, output_attentions=True, output_hidden_states=True, return_dict=True):\n",
    "\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n",
    "            )\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = (\n",
    "            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "        )\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(\n",
    "                ((batch_size, seq_length + past_key_values_length))\n",
    "            )\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=self.dtype)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n",
    "            attention_mask, input_shape, past_key_values_length\n",
    "        )\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            (\n",
    "                encoder_batch_size,\n",
    "                encoder_sequence_length,\n",
    "                _,\n",
    "            ) = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(\n",
    "                encoder_attention_mask\n",
    "            )\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        # if hasattr(self, \"embeddings_project\"):\n",
    "        #     embedding_output = self.embeddings_project(embedding_output)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "        pooled_output = self.pooler(sequence_output) if self.add_pooling_layer else None\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        return DataClassRoformer(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "        # return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "        \n",
    "\n",
    "    # 添加了个past_key_values_length\n",
    "    def get_extended_attention_mask(self, attention_mask: jt.Var, input_shape, past_key_values_length):\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            if self.config.is_decoder and past_key_values_length > 0: # 第一次编码的时候不需要使用decoder mask，之后的需要decoder mask。\n",
    "                extended_attention_mask = self.create_extended_attention_mask_for_decoder(\n",
    "                    input_shape, attention_mask\n",
    "                )\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None):\n",
    "        batch_size, seq_length = input_shape\n",
    "        seq_ids = torch.arange(seq_length)\n",
    "        causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "        # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "        # causal and attention masks must have same type with pytorch version < 1.3\n",
    "        causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "        if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "            prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "            causal_mask = torch.cat(\n",
    "                [\n",
    "                    torch.ones((batch_size, seq_length, prefix_seq_len), dtype=causal_mask.dtype),\n",
    "                    causal_mask,\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "        extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "        return extended_attention_mask\n",
    "    \n",
    "    def get_head_mask(\n",
    "        self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Prepare the head mask if needed.\n",
    "\n",
    "        Args:\n",
    "            head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n",
    "                The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
    "            num_hidden_layers (`int`):\n",
    "                The number of hidden layers in the model.\n",
    "            is_attention_chunked (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the attentions scores are computed by chunks or not.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor` with shape `[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or list with\n",
    "            `[None]` for each layer.\n",
    "        \"\"\"\n",
    "        if head_mask is not None:\n",
    "            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n",
    "            if is_attention_chunked is True:\n",
    "                head_mask = head_mask.unsqueeze(-1)\n",
    "        else:\n",
    "            head_mask = [None] * num_hidden_layers\n",
    "\n",
    "        return head_mask\n",
    "    \n",
    "    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):\n",
    "        \"\"\"-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\"\"\"\n",
    "        if head_mask.dim() == 1:\n",
    "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            head_mask = head_mask.expand(num_hidden_layers, -1, -1, -1, -1)\n",
    "        elif head_mask.dim() == 2:\n",
    "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
    "        assert head_mask.dim() == 5, f\"head_mask.dim != 5, instead {head_mask.dim()}\"\n",
    "        head_mask = head_mask.to(dtype=self.dtype)  # switch to float if need + fp16 compatibility\n",
    "        return head_mask\n",
    "\n",
    "# DONE\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n",
    "\n",
    "        # LayerNorm\n",
    "        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps) if config.norm_type == 'layer_norm' else Norm(eps=config.layer_norm_eps)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, inputs_embeds=None):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(\n",
    "                input_shape, dtype=torch.to(torch.Long) , device=inputs_embeds.device\n",
    "            ) # torch.long is equivalent to to(torch.int64)\n",
    "\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter): # TODO: change to just jt.Var?\n",
    "        \"\"\"\n",
    "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
    "        the 2nd half of the vector. [dim // 2:]\n",
    "        \"\"\"\n",
    "        n_pos, dim = out.shape\n",
    "        position_enc = np.array(\n",
    "            [\n",
    "                [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "                for pos in range(n_pos)\n",
    "            ]\n",
    "        )\n",
    "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
    "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "        out[:, 0:sentinel] = torch.float16(np.sin(position_enc[:, 0::2]))\n",
    "        out[:, sentinel:] = torch.float16(np.cos(position_enc[:, 1::2]))\n",
    "        out.detach_inplace()\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, seq_len: int, past_key_values_length: int = 0):\n",
    "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
    "        positions = torch.arange(\n",
    "            start=past_key_values_length,\n",
    "            end=past_key_values_length + seq_len,\n",
    "            dtype=torch.Long,\n",
    "        )\n",
    "        return super().forward(positions)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            config.hidden_size // config.num_attention_heads,\n",
    "        )\n",
    "        self.layer = nn.ModuleList(\n",
    "            [Layer(config) for _ in range(12)]\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = (\n",
    "            () if output_attentions and self.config.add_cross_attention else None\n",
    "        )\n",
    "\n",
    "        past_key_values_length = (\n",
    "            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "        )\n",
    "\n",
    "        sinusoidal_pos = self.embed_positions(hidden_states.shape[1], past_key_values_length)[\n",
    "            None, None, :, :\n",
    "        ].chunk(2, dim=-1)\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                sinusoidal_pos,\n",
    "                layer_head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        # if not return_dict:\n",
    "        # return tuple(\n",
    "        #     v\n",
    "        #     for v in [\n",
    "        #         hidden_states,\n",
    "        #         next_decoder_cache,\n",
    "        #         all_hidden_states,\n",
    "        #         all_self_attentions,\n",
    "        #         all_cross_attentions,\n",
    "        #     ]\n",
    "        #     if v is not None\n",
    "        # )\n",
    "        return DataClassEncoder(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Layer, self).__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = Attention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(\n",
    "                    f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "                )\n",
    "            self.crossattention = Attention(config)\n",
    "        self.intermediate = Intermediate(config)\n",
    "        self.output = Output(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, sinusoidal_pos=None,\n",
    "                head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None,\n",
    "                past_key_value=None, output_attentions=False):\n",
    "        # attention_output = self.attention(hidden_states, attention_mask)\n",
    "        # intermediate_output = self.intermediate(attention_output)\n",
    "        # layer_output = self.output(intermediate_output, attention_output)\n",
    "        # return layer_output\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = (\n",
    "            past_key_value[:2] if past_key_value is not None else None\n",
    "        )\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            sinusoidal_pos,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[\n",
    "                1:\n",
    "            ]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention \"\n",
    "                    \"layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = (\n",
    "                past_key_value[-2:] if past_key_value is not None else None\n",
    "            )\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                sinusoidal_pos,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = (\n",
    "                outputs + cross_attention_outputs[1:-1]\n",
    "            )  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = self.apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk,\n",
    "            self.chunk_size_feed_forward,\n",
    "            self.seq_len_dim,\n",
    "            attention_output,\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def apply_chunking_to_forward(self,\n",
    "        forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This function chunks the `input_tensors` into smaller input tensor parts of size `chunk_size` over the dimension\n",
    "        `chunk_dim`. It then applies a layer `forward_fn` to each chunk independently to save memory.\n",
    "\n",
    "        If the `forward_fn` is independent across the `chunk_dim` this function will yield the same result as directly\n",
    "        applying `forward_fn` to `input_tensors`.\n",
    "\n",
    "        Args:\n",
    "            forward_fn (`Callable[..., torch.Tensor]`):\n",
    "                The forward function of the model.\n",
    "            chunk_size (`int`):\n",
    "                The chunk size of a chunked tensor: `num_chunks = len(input_tensors[0]) / chunk_size`.\n",
    "            chunk_dim (`int`):\n",
    "                The dimension over which the `input_tensors` should be chunked.\n",
    "            input_tensors (`Tuple[torch.Tensor]`):\n",
    "                The input tensors of `forward_fn` which will be chunked\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: A tensor with the same shape as the `forward_fn` would have given if applied`.\n",
    "\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        # rename the usual forward() fn to forward_chunk()\n",
    "        def forward_chunk(self, hidden_states):\n",
    "            hidden_states = self.decoder(hidden_states)\n",
    "            return hidden_states\n",
    "\n",
    "\n",
    "        # implement a chunked forward function\n",
    "        def forward(self, hidden_states):\n",
    "            return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n",
    "        ```\"\"\"\n",
    "\n",
    "        assert len(input_tensors) > 0, f\"{input_tensors} has to be a tuple/list of tensors\"\n",
    "\n",
    "        # inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\n",
    "        num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n",
    "        if num_args_in_forward_chunk_fn != len(input_tensors):\n",
    "            raise ValueError(\n",
    "                f\"forward_chunk_fn expects {num_args_in_forward_chunk_fn} arguments, but only {len(input_tensors)} input \"\n",
    "                \"tensors are given\"\n",
    "            )\n",
    "\n",
    "        if chunk_size > 0:\n",
    "            tensor_shape = input_tensors[0].shape[chunk_dim]\n",
    "            for input_tensor in input_tensors:\n",
    "                if input_tensor.shape[chunk_dim] != tensor_shape:\n",
    "                    raise ValueError(\n",
    "                        f\"All input tenors have to be of the same shape: {tensor_shape}, \"\n",
    "                        f\"found shape {input_tensor.shape[chunk_dim]}\"\n",
    "                    )\n",
    "\n",
    "            if input_tensors[0].shape[chunk_dim] % chunk_size != 0:\n",
    "                raise ValueError(\n",
    "                    f\"The dimension to be chunked {input_tensors[0].shape[chunk_dim]} has to be a multiple of the chunk \"\n",
    "                    f\"size {chunk_size}\"\n",
    "                )\n",
    "\n",
    "            num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n",
    "\n",
    "            # chunk input tensor into tuples\n",
    "            input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n",
    "            # apply forward fn to every tuple\n",
    "            output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n",
    "            # concatenate output at same dimension\n",
    "            return torch.cat(output_chunks, dim=chunk_dim)\n",
    "\n",
    "        return forward_fn(*input_tensors)\n",
    "    \n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "        \n",
    "# DONE?\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Attention, self).__init__()\n",
    "        self.self = SelfAttention(config)\n",
    "        self.output = SelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_linear_layer(self, layer, index, dim: int = 0) -> nn.Linear:\n",
    "        \"\"\"\n",
    "        Prune a linear layer to keep only entries in index.\n",
    "\n",
    "        Used to remove heads.\n",
    "\n",
    "        Args:\n",
    "            layer (`jt.nn.Linear`): The layer to prune.\n",
    "            index (`jt.int64`): The indices to keep in the layer.\n",
    "            dim (`int`, *optional*, defaults to 0): The dimension on which to keep the indices.\n",
    "\n",
    "        Returns:\n",
    "            `torch.nn.Linear`: The pruned layer as a new layer with `requires_grad=True`.\n",
    "        \"\"\"\n",
    "        W = layer.weight.index_select(dim, index).clone().detach()\n",
    "        if layer.bias is not None:\n",
    "            if dim == 1:\n",
    "                b = layer.bias.clone().detach()\n",
    "            else:\n",
    "                b = layer.bias[index].clone().detach()\n",
    "        new_size = list(layer.weight.size())\n",
    "        new_size[dim] = len(index)\n",
    "        new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None)\n",
    "        new_layer.weight.requires_grad = False\n",
    "        new_layer.weight.copy_(W.contiguous())\n",
    "        new_layer.weight.requires_grad = True\n",
    "        if layer.bias is not None:\n",
    "            new_layer.bias.requires_grad = False\n",
    "            new_layer.bias.copy_(b.contiguous())\n",
    "            new_layer.bias.requires_grad = True\n",
    "        return new_layer\n",
    "\n",
    "\n",
    "    def find_pruneable_heads_and_indices(\n",
    "            heads: List[int], \n",
    "            n_heads: int, \n",
    "            head_size: int, \n",
    "            already_pruned_heads: Set[int]\n",
    "    ) -> Tuple[Set[int], torch.Long]:\n",
    "        \"\"\"\n",
    "        Finds the heads and their indices taking :obj:`already_pruned_heads` into account.\n",
    "        Args:\n",
    "        heads (`List[int]`): List of the indices of heads to prune.\n",
    "        n_heads (`int`): The number of heads in the model.\n",
    "        head_size (`int`): The size of each head.\n",
    "        already_pruned_heads (`Set[int]`): A set of already pruned heads.\n",
    "\n",
    "        Returns:\n",
    "            `Tuple[Set[int], torch.LongTensor]`: A tuple with the indices of heads to prune taking `already_pruned_heads`\n",
    "            into account and the indices of rows/columns to keep in the layer weight.\n",
    "        \"\"\"\n",
    "        mask = torch.ones(n_heads, head_size)\n",
    "        heads = set(heads) - already_pruned_heads\n",
    "        for head in heads:\n",
    "            head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n",
    "            mask[head] = 0\n",
    "        mask = mask.view(-1).contiguous().eq(1)\n",
    "        index = torch.arange(len(mask))[mask].long()\n",
    "        return heads, index\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = self.find_pruneable_heads_and_indices(\n",
    "            heads,\n",
    "            self.self.num_attention_heads,\n",
    "            self.self.attention_head_size,\n",
    "            self.pruned_heads,\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = self.prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = self.prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = self.prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = self.prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = (\n",
    "            self.self.attention_head_size * self.self.num_attention_heads\n",
    "        )\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        sinusoidal_pos=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            sinusoidal_pos,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[\n",
    "            1:\n",
    "        ]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# DONE? (should be, i need to run it)\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.use_bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "        self.is_decoder = True # TODO: CHECK config.is_decoder\n",
    "        self.rotary_value = config.rotary_value\n",
    "\n",
    "    # reshape and permute the dims to prepare for the attention head splitting\n",
    "    def transpose_for_scores(self, x: torch.Tensor):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.reshape(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        sinusoidal_pos=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # rotary query\n",
    "        query_layer = self.apply_rotary(query_layer, sinusoidal_pos)\n",
    "\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = self.apply_rotary(key_layer, sinusoidal_pos)\n",
    "            if self.rotary_value:\n",
    "                value_layer = self.apply_rotary(value_layer, sinusoidal_pos)\n",
    "            key_layer = torch.concat([past_key_value[0], key_layer], dim=-2)\n",
    "            value_layer = torch.concat([past_key_value[1], value_layer], dim=-2)\n",
    "\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = self.apply_rotary(key_layer, sinusoidal_pos)\n",
    "            if self.rotary_value:\n",
    "                value_layer = self.apply_rotary(value_layer, sinusoidal_pos)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None: \n",
    "            # print(\"ATTENTION MASK: \", attention_mask.shape)\n",
    "            # print(\"ATTENTION SCORES: \", attention_scores.shape)\n",
    "            # Apply the attention mask\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # context layer\n",
    "        context_layer: torch.Tensor = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = torch.reshape(context_layer.permute(0, 2, 1, 3), hidden_states.shape) # TODO: CHECK\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = torch.reshape(context_layer, *new_context_layer_shape) # TODO: CHECK\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_rotary(x, sinusoidal_pos):\n",
    "        sin, cos = sinusoidal_pos\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        x1_rot = x1 * cos - x2 * sin\n",
    "        x2_rot = x1 * sin + x2 * cos\n",
    "        x = torch.stack([x1_rot, x2_rot], dim=-1)\n",
    "        x = x.flatten(-2, -1)\n",
    "        return x\n",
    "\n",
    "# DONE\n",
    "class SelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size, bias=config.use_bias)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) if config.norm_type == 'layer_norm' else Norm(eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "# DONE\n",
    "class Output(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Output, self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.use_bias)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) if config.norm_type == 'layer_norm' else Norm(eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# DONE\n",
    "class Intermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Intermediate, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.use_bias)\n",
    "        self.intermediate_act_fn = nn.GELU() # just using GELU instead of ACT2FN[config.hidden_act]\n",
    "\n",
    "    def foward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "# DONE\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, eps):\n",
    "        super(Norm, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        variance = torch.mean(x**2, dim=-1, keepdims=True)\n",
    "        x = x * torch.rsqrt(variance + self.eps)\n",
    "        return x\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Only MLM Head\n",
    "class Cls(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Cls, self).__init__()\n",
    "        self.predictions = RoFormerLMPredictionHead(config) if config.norm_type==\"layer_norm\" else RoFormerV2LMPredictionHead(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.predictions(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "\n",
    "class RoFormerV2LMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(config.embedding_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.decoder(hidden_states)\n",
    "\n",
    "class RoFormerLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(RoFormerLMPredictionHead, self).__init__()\n",
    "        self.transform = PredictionHeadTransform(config)\n",
    "        # Assuming the output dimension for the decoder matches the vocabulary size or embedding_num\n",
    "        self.decoder = nn.Linear(config.embedding_size, config.vocab_size, bias=False)\n",
    "        self.bias = torch.zeros(config.vocab_size)\n",
    "\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class PredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PredictionHeadTransform, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n",
    "        self.transform_act_fn = nn.GELU()\n",
    "        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class RoFormerPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        # self.activation = ACT2FN[config.pooler_activation]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        # pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RoFormerTokenizer, RoFormerForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pdb\n",
    "import time\n",
    "import jieba\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def load_data(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    return data\n",
    "\n",
    "pretrained_model = \"/root/2021080087/roformer_ann_2023/src/roformer_chinese_base/pytorch_model.bin\"\n",
    "config_path = \"/root/2021080087/roformer_ann_2023/src/roformer_chinese_base/config.json\"\n",
    "vocab = \"/root/2021080087/roformer_ann_2023/src/roformer_chinese_base/vocab.txt\"\n",
    "\n",
    "config = json.load(open(config_path))\n",
    "# convert config into an object\n",
    "config = type('', (), config)()\n",
    "config.is_decoder = True\n",
    "config.add_cross_attention = True\n",
    "config.chunk_size_feed_forward = 0\n",
    "config.add_pooling_layer = True\n",
    "config.norm_type = \"layer_norm\"\n",
    "config.use_bias = True\n",
    "config.rotary_value = False\n",
    "config.use_cache = True\n",
    "config.num_labels = 2\n",
    "\n",
    "# set up tokenizer\n",
    "tokenizer = RoFormerTokenizer.from_pretrained(vocab)\n",
    "\n",
    "# model = ModelSeqClassifier(config)\n",
    "model = RoFormerForSequenceClassification.from_pretrained(pretrained_model, config=config_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# model.load(pretrained_model)\n",
    "\n",
    "# data loading\n",
    "test_path = \"/root/2021080087/roformer_ann_2023/CAIL2019-SCM/test.json\"\n",
    "train_path = \"/root/2021080087/roformer_ann_2023/CAIL2019-SCM/train.json\"\n",
    "valid_path = \"/root/2021080087/roformer_ann_2023/CAIL2019-SCM/valid.json\"\n",
    "test_data = load_data(test_path)\n",
    "train_data = load_data(train_path)\n",
    "valid_data = load_data(valid_path)\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "def train(model, data, optimizer, loss_fn, epoch):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data):\n",
    "        optimizer.zero_grad() # encode_plus?\n",
    "        # Prepare batch data\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "\n",
    "        for sample in batch:\n",
    "            tokenized = tokenizer(\n",
    "                sample['A'] + tokenizer.sep_token + sample['B'] + tokenizer.sep_token + sample['C'], \n",
    "                padding=\"max_length\", \n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,  # Define MAX_LENGTH according to your model's requirements\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids.append(tokenized[\"input_ids\"][0])\n",
    "            attention_mask.append(tokenized[\"attention_mask\"][0])\n",
    "            labels.append(0 if sample['label'] == 'B' else 1)\n",
    "            \n",
    "        input_ids = torch.stack(input_ids).to(device)\n",
    "        attention_mask = torch.stack(attention_mask).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(batch), len(data)*batch_size,\n",
    "                    100. * batch_idx / len(data), loss.item()))\n",
    "    print(\"Average loss: \", total_loss / len(data))\n",
    "\n",
    "def test(model, data, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data):\n",
    "            # Prepare batch data\n",
    "            input_ids = []\n",
    "            attention_mask = []\n",
    "            labels = []\n",
    "\n",
    "            for sample in batch:\n",
    "                tokenized = tokenizer(\n",
    "                    sample['A'] + tokenizer.sep_token + sample['B'] + tokenizer.sep_token + sample['C'], \n",
    "                    padding=\"max_length\", \n",
    "                    truncation=True,\n",
    "                    max_length=MAX_LENGTH,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                input_ids.append(tokenized[\"input_ids\"][0])\n",
    "                attention_mask.append(tokenized[\"attention_mask\"][0])\n",
    "                labels.append(0 if sample['label'] == 'B' else 1)\n",
    "\n",
    "            input_ids = torch.stack(input_ids).to(device)\n",
    "            attention_mask = torch.stack(attention_mask).to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            total_samples += len(batch)\n",
    "\n",
    "            # Optionally calculate accuracy or other metrics here\n",
    "            pred = outputs.logits.argmax(dim=1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "\n",
    "        test_loss /= total_samples\n",
    "        accuracy = 100. * correct / total_samples\n",
    "\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{total_samples} ({accuracy:.0f}%)')\n",
    "\n",
    "# train\n",
    "epochs = 5\n",
    "lr = 2e-5\n",
    "batch_size = 8\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "start_time = time.time()\n",
    "\n",
    "train_batches = list(batch_generator(train_data, batch_size))\n",
    "valid_batches = list(batch_generator(valid_data, batch_size))\n",
    "test_batches = list(batch_generator(test_data, batch_size))\n",
    "\n",
    "# pdb.set_trace()\n",
    "test(model, valid_batches, loss_fn)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_batches, optimizer, loss_fn, epoch)\n",
    "    # if epoch % 5 == 0:\n",
    "    test(model, valid_batches, loss_fn)\n",
    "    print(\"Minutes elapsed: \", (time.time() - start_time) / 60)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
